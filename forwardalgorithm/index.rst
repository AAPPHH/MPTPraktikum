Der VorwÃ¤rts-Algorithmus
========================

In diesem Praktikum implementieren Sie den VorwÃ¤rts-Algorithmus am Beispiel
von Ã¼berlieferten Texten einer fiktiven uralten Zivilisation.

ArchÃ¤ologie - Das Setting
-------------------------

Bei einer Ausgrabung finden Sie fÃ¼nf Texte einer uralten Zivilisation. 
Diese Texte lauten wir folgt

**Erster Text**:

.. code-block:: bash

  ğŸ§ğŸ§¯  ğŸ¤‡ğŸ¥ŒğŸ¦Ÿ  ğŸ¤‡ğŸ¤«ğŸ¥ŒğŸ˜¼ğŸ¤«  ğŸ¤‡ğŸ¤Ÿ  ğŸ¦˜ğŸ¦ŠğŸ¥¾
  ğŸ¤¯ğŸ¤¯ğŸ§›ğŸ˜ğŸ˜¼  ğŸ¥ŒğŸ¥¦ğŸ¤‡ğŸ¤‡ğŸ¦ŸğŸ˜’  ğŸ§ğŸ§›ğŸ§ğŸ¦ŠğŸ§¯  ğŸ¦˜ğŸ˜ğŸ¦˜ğŸ¦ŸğŸ¤¯ğŸ¦º  ğŸ¥¾ğŸ¥¾ğŸ¥‘ğŸ¥‘ğŸ¥Œ
  ğŸ¤ŸğŸ§ğŸ¥¦  ğŸ¥¦ğŸ˜¼ğŸ¦ŠğŸ¥¾ğŸ¦ºğŸ¤‡  ğŸ§¯ğŸ¦ŸğŸ¦˜ğŸ¤¶ğŸ¦ºğŸ¦ŸğŸ¥·ğŸ§¯  ğŸ˜ğŸ¦ŸğŸ¦ŠğŸ¤Ÿ
  ğŸ§ğŸ˜ğŸ¦ºğŸ¦ŠğŸ¤«ğŸ¥ŒğŸ§¯  ğŸ¦ºğŸ˜  ğŸ¦˜ğŸ˜¼ğŸ¥¦ğŸ˜ğŸ¥‘  ğŸ¤¯ğŸ¦˜ğŸ¦˜ğŸ¦ºğŸ¦º  ğŸ˜¼ğŸ¤ŸğŸ¥¾ğŸ¤¯ğŸ¥¾ğŸ¤¶ğŸ¥¦ğŸ¦˜
  ğŸ¦ŸğŸ˜ğŸ¤ŸğŸ¦ºğŸ˜¼ğŸ¤¶ğŸ¤Ÿ  ğŸ§ğŸ¦ŸğŸ˜ğŸ¤¯ğŸ§›ğŸ¤‡ğŸ¤Ÿ  ğŸ§¯ğŸ¤Ÿ  ğŸ˜ğŸ¦˜ğŸ¥ŒğŸ§ğŸ¦˜ğŸ¤¶ğŸ¦ºğŸ¦Š  ğŸ¤¶ğŸ¤¶ğŸ¤«ğŸ¤ŸğŸ¦º
  ğŸ¤‡ğŸ¥ŒğŸ˜ğŸ¥ŒğŸ¤ŸğŸ¤‡  ğŸ¥¦ğŸ¦ºğŸ¦˜ğŸ¤‡ğŸ¥¾ğŸ¥·  ğŸ¤¯ğŸ¦˜ğŸ¦ºğŸ˜’  ğŸ§¯ğŸ¦ŸğŸ¦Š  ğŸ¤‡ğŸ§¯ğŸ¥Œ  ğŸ¦ŸğŸ¤‡ğŸ§›ğŸ§›ğŸ¤ŸğŸ¤¯ğŸ§›
  ğŸ¤«ğŸ¦Ÿ  ğŸ¥¦ğŸ¤¶ğŸ¥‘ğŸ¤‡ğŸ¥‘  ğŸ¥¦ğŸ¦ŠğŸ¥¦  ğŸ˜¼ğŸ¤‡ğŸ¦ŸğŸ˜ğŸ¥‘  ğŸ§ğŸ¦ŠğŸ¤¯ğŸ¤«ğŸ¦˜ğŸ¥¾ğŸ¦Š
  ğŸ˜ğŸ˜¼  ğŸ§›ğŸ˜ğŸ§›ğŸ¦ŠğŸ¤‡ğŸ¦ŠğŸ¦ºğŸ¥·  ğŸ¤¯ğŸ¦˜ğŸ¤¶ğŸ§ğŸ§ğŸ§ğŸ¥Œ  ğŸ¥·ğŸ¤¶ğŸ¦ŠğŸ¦ŠğŸ˜’ğŸ§›  ğŸ˜¼ğŸ˜ğŸ¤«ğŸ§

**Zweiter Text**:

.. code-block:: bash

  ğŸ¦ºğŸ¤¶ğŸ¦ºğŸ¦Ÿ  ğŸ¦˜ğŸ§¯  ğŸ§›ğŸ¥‘ğŸ¤¯  ğŸ¤¶ğŸ¥ŒğŸ¤‡ğŸ¥·ğŸ˜’
  ğŸ§ğŸ¦˜ğŸ¥¾  ğŸ˜ğŸ¤¯  ğŸ¤«ğŸ§ğŸ¤ŸğŸ¦ºğŸ¤ŸğŸ§¯  ğŸ¦ºğŸ¤ŸğŸ¦ŸğŸ¤«ğŸ˜¼
  ğŸ¤¶ğŸ˜’ğŸ¦ºğŸ¥¾  ğŸ§ğŸ¤¶ğŸ¥·ğŸ§›ğŸ§›ğŸ¤«ğŸ§›  ğŸ¤‡ğŸ¥¾ğŸ¤«ğŸ¤‡ğŸ˜’ğŸ¦ŸğŸ¦Ÿ  ğŸ¤‡ğŸ¥¦ğŸ¥·ğŸ§ğŸ¥‘
  ğŸ˜ğŸ§¯ğŸ¦˜ğŸ¦Ÿ  ğŸ§›ğŸ¦ŸğŸ¤‡ğŸ§›ğŸ¤«  ğŸ˜¼ğŸ¤ŸğŸ¦˜  ğŸ˜ğŸ¦ŸğŸ¤¯
  ğŸ¥·ğŸ¤¯ğŸ¤ŸğŸ§¯ğŸ§  ğŸ§ğŸ¤¶ğŸ§ğŸ§›ğŸ¦˜  ğŸ§ğŸ¤¶ğŸ§¯ğŸ¥‘ğŸ¤¯ğŸ˜’  ğŸ¥¦ğŸ¥¾ğŸ¤¶ğŸ¤«ğŸ˜  ğŸ˜ğŸ˜ğŸ¤¯ğŸ˜¼ğŸ¤¶ğŸ§ğŸ¥·ğŸ¦Ÿ
  ğŸ§›ğŸ˜¼ğŸ¤Ÿ  ğŸ¥‘ğŸ¤‡ğŸ¥¾  ğŸ¥¾ğŸ˜’ğŸ¤«  ğŸ§ğŸ˜ğŸ§›ğŸ¤ŸğŸ¦ºğŸ¥‘ğŸ¤¯ğŸ¤‡  ğŸ¦ŸğŸ§¯ğŸ¥·  ğŸ¥¾ğŸ¤‡ğŸ¥¾
  ğŸ¥¦ğŸ§ğŸ¦ŠğŸ˜ğŸ§ğŸ˜’  ğŸ˜ğŸ¤‡ğŸ˜¼ğŸ¤¯ğŸ¦ŠğŸ¥¾ğŸ¥·  ğŸ˜ğŸ¥ŒğŸ¤¯ğŸ¦ŸğŸ§›ğŸ§ğŸ§›  ğŸ¦˜ğŸ§  ğŸ˜ğŸ˜’ğŸ¦ŠğŸ§›  ğŸ¥¾ğŸ¥¾ğŸ¤¶ğŸ¤¯
  ğŸ˜ğŸ˜’ğŸ§›ğŸ¤¯ğŸ§  ğŸ¤ŸğŸ¦ºğŸ¤¯ğŸ§›  ğŸ¦ŠğŸ¤¶ğŸ¥¾ğŸ¥‘ğŸ¥·ğŸ¦˜ğŸ¦º  ğŸ¦ŸğŸ˜¼ğŸ¦˜  ğŸ¦˜ğŸ¤¶ğŸ˜¼ğŸ¤«  ğŸ¦ŸğŸ¦˜ğŸ˜¼ğŸ¤¯ğŸ¤¯

**Dritter Text**:

.. code-block:: bash

  ğŸ§¯ğŸ§›ğŸ¦ŸğŸ¤«ğŸ¥¦ğŸ¤‡ğŸ¦Ÿ  ğŸ˜ğŸ¥‘ğŸ¤ŸğŸ¥·ğŸ§¯ğŸ¤«ğŸ¤«  ğŸ¥·ğŸ§¯ğŸ¦ºğŸ¦º  ğŸ§›ğŸ¥‘ğŸ¥¾ğŸ¦ºğŸ§
  ğŸ¤‡ğŸ¦ŸğŸ§ğŸ¥ŒğŸ¤‡ğŸ¥¾ğŸ¤¶ğŸ¥‘  ğŸ¥¾ğŸ¤«  ğŸ˜¼ğŸ¥‘ğŸ¦º  ğŸ¤¶ğŸ¦ºğŸ¦Ÿ  ğŸ¤«ğŸ¦º  ğŸ¥¦ğŸ˜¼ğŸ§¯ğŸ¤«ğŸ¦ºğŸ¤¶
  ğŸ¤ŸğŸ¥¾ğŸ¦ºğŸ¤‡  ğŸ¤¯ğŸ¥¦ğŸ¤«ğŸ¦ŸğŸ¤¯ğŸ¤ŸğŸ¤ŸğŸ¤¶  ğŸ§›ğŸ¥¦ğŸ§  ğŸ¦˜ğŸ§¯ğŸ¥¾ğŸ§›ğŸ§¯  ğŸ¥‘ğŸ¤¶ğŸ˜¼
  ğŸ˜ğŸ˜ğŸ¥¾ğŸ§›ğŸ§›  ğŸ¦˜ğŸ¥‘ğŸ§ğŸ˜¼ğŸ¥ŒğŸ¤«ğŸ¤Ÿ  ğŸ¤ŸğŸ¤‡ğŸ¤¶ğŸ¦ŠğŸ¥¾  ğŸ¤«ğŸ˜¼ğŸ¤«ğŸ¥¾ğŸ¦ŸğŸ§  ğŸ¤‡ğŸ¦˜ğŸ¦ºğŸ§›ğŸ˜¼  ğŸ¥¾ğŸ¤‡ğŸ¥ŒğŸ§ğŸ¤«ğŸ¥·ğŸ¦ŸğŸ¤«
  ğŸ¥‘ğŸ§ğŸ§ğŸ¤«ğŸ¥‘ğŸ¦ŸğŸ¤ŸğŸ¥‘  ğŸ§›ğŸ¥ŒğŸ¥¾  ğŸ˜¼ğŸ˜¼ğŸ¤¶ğŸ¤ŸğŸ¦˜  ğŸ¤ŸğŸ¦ºğŸ¤‡ğŸ¦˜ğŸ§›ğŸ¤¯ğŸ¥Œ  ğŸ§ğŸ¦ŸğŸ¤¯ğŸ¥¾
  ğŸ¦˜ğŸ¥‘  ğŸ˜¼ğŸ¥¦ğŸ¦ŸğŸ§›ğŸ¥‘ğŸ§¯ğŸ¥Œ  ğŸ¤‡ğŸ˜¼  ğŸ˜¼ğŸ¤«
  ğŸ¥¾ğŸ¦ŠğŸ¥·  ğŸ¦˜ğŸ¤Ÿ  ğŸ¦ŠğŸ¥‘ğŸ¤¯ğŸ¤«ğŸ¥ŒğŸ§›  ğŸ¤‡ğŸ¤«ğŸ˜ğŸ¦ºğŸ¥Œ  ğŸ¥¾ğŸ¥¦ğŸ¥‘ğŸ¦ŠğŸ§  ğŸ¥ŒğŸ˜¼ğŸ˜ğŸ¤«ğŸ¤¯
  ğŸ¥¦ğŸ¤¶ğŸ§›ğŸ¤ŸğŸ˜’ğŸ¦ŸğŸ§ğŸ§¯  ğŸ¤‡ğŸ¤«ğŸ§›ğŸ¥¦ğŸ¦ºğŸ¤Ÿ  ğŸ¥¦ğŸ¤¶ğŸ¥‘ğŸ¦Ÿ  ğŸ¦ºğŸ§›ğŸ¦˜ğŸ¥¦ğŸ§¯ğŸ¦º

**Vierter Text**:

.. code-block:: bash

  ğŸ˜ğŸ§ğŸ¥¦ğŸ¦ŠğŸ¦ŸğŸ¥¦  ğŸ¤¶ğŸ§¯ğŸ¥ŒğŸ§¯ğŸ¤Ÿ  ğŸ§¯ğŸ¦ºğŸ§¯ğŸ˜’ğŸ§ğŸ§›  ğŸ¦ŠğŸ§ğŸ¥‘ğŸ¤«ğŸ¤¶  ğŸ¤¶ğŸ§ğŸ¦ºğŸ¦ŠğŸ˜ğŸ¤«ğŸ§›ğŸ¥·  ğŸ˜’ğŸ¤‡ğŸ¥ŒğŸ¤¯ğŸ¤‡ğŸ¤¶ğŸ¤¶ğŸ¥¾
  ğŸ¦ŠğŸ˜¼ğŸ¦ŸğŸ¦˜ğŸ˜ğŸ§›ğŸ¤Ÿ  ğŸ˜ğŸ¤‡ğŸ¤¯ğŸ§›ğŸ¤‡ğŸ¦˜ğŸ¦˜ğŸ˜¼  ğŸ¥‘ğŸ¥·ğŸ¥‘ğŸ¤‡ğŸ¤ŸğŸ¤‡  ğŸ¦ŠğŸ¥‘ğŸ¥ŒğŸ˜¼ğŸ¦˜ğŸ¤‡  ğŸ˜¼ğŸ¤Ÿ  ğŸ¦ºğŸ¦ŸğŸ¥·ğŸ˜’ğŸ¥¦ğŸ¤¯
  ğŸ¤‡ğŸ¦Ÿ  ğŸ§›ğŸ¤ŸğŸ¥¾ğŸ¤¯ğŸ¥¦  ğŸ¥¦ğŸ˜’ğŸ˜ğŸ¥‘ğŸ¥·ğŸ¤ŸğŸ¥¾  ğŸ¥‘ğŸ¤¶  ğŸ¦ºğŸ¤«ğŸ¥·ğŸ˜’ğŸ¦ºğŸ¤Ÿ  ğŸ¥¦ğŸ˜’ğŸ§›ğŸ§¯ğŸ¤‡ğŸ¥‘ğŸ¤¶ğŸ¥¾
  ğŸ˜ğŸ¤ŸğŸ¦ºğŸ˜’ğŸ¥¾ğŸ¤‡ğŸ¤¯ğŸ¤¶  ğŸ¥¦ğŸ¦ŸğŸ¥¦ğŸ¤ŸğŸ¤«ğŸ¤¯ğŸ¥‘ğŸ¤‡  ğŸ¤ŸğŸ¦ŠğŸ¥¾ğŸ¤ŸğŸ¤¶ğŸ§¯  ğŸ˜¼ğŸ¤¯ğŸ˜’  ğŸ¥¦ğŸ¤‡ğŸ§
  ğŸ˜’ğŸ¤ŸğŸ¥ŒğŸ¥ŒğŸ¤«ğŸ§›ğŸ˜¼  ğŸ¤¯ğŸ¦ŸğŸ¥‘ğŸ¤‡ğŸ¥‘  ğŸ¥‘ğŸ¦ŸğŸ˜ğŸ˜¼ğŸ˜  ğŸ¦˜ğŸ¦ŸğŸ¥¦ğŸ¤‡ğŸ¦˜
  ğŸ¤¯ğŸ˜ğŸ¥¦ğŸ¦ŠğŸ˜¼ğŸ˜ğŸ¤¶ğŸ¤‡  ğŸ˜¼ğŸ¤¯ğŸ¤¯ğŸ˜ğŸ¥¾ğŸ˜¼ğŸ¥·  ğŸ¤¯ğŸ¥‘ğŸ¦ŸğŸ˜¼  ğŸ¥·ğŸ˜ğŸ¤‡ğŸ¥Œ  ğŸ¦ºğŸ¦ºğŸ¦ºğŸ¥ŒğŸ¦˜ğŸ§›ğŸ¤«ğŸ¤‡
  ğŸ¥·ğŸ§ğŸ¥¦ğŸ˜  ğŸ§ğŸ¤‡ğŸ˜’ğŸ¦ŠğŸ¤¯ğŸ¤¯ğŸ˜ğŸ˜¼  ğŸ§¯ğŸ§ğŸ¥¦  ğŸ˜ğŸ¤¯  ğŸ¦ŸğŸ¤«ğŸ¥‘ğŸ¤ŸğŸ¤«ğŸ¦Ÿ  ğŸ˜¼ğŸ§ğŸ¦ºğŸ˜’ğŸ˜’ğŸ¦ºğŸ¤«ğŸ¥¦
  ğŸ§›ğŸ¥¦ğŸ¦ŸğŸ˜¼ğŸ˜’ğŸ˜’ğŸ¦˜  ğŸ¤‡ğŸ¥¾  ğŸ¥ŒğŸ¥¦ğŸ§›ğŸ¤¯ğŸ¤¶  ğŸ¦ŸğŸ¥·ğŸ¤‡ğŸ¥ŒğŸ˜’ğŸ¤¶  ğŸ¦ŸğŸ¤¯ğŸ˜¼ğŸ˜¼

**FÃ¼nfter Text**:

.. code-block:: bash

  ğŸ¥ŒğŸ¦ŠğŸ¥¾ğŸ¦ŠğŸ§ğŸ§¯ğŸ§›ğŸ¤¯  ğŸ§›ğŸ˜ğŸ¦ŸğŸ¥ŒğŸ¦˜ğŸ¥¾ğŸ¤¯  ğŸ˜ğŸ¤¶ğŸ§ğŸ¥ŒğŸ¦˜ğŸ¤‡  ğŸ§›ğŸ¦ŸğŸ¤¯  ğŸ¦ŠğŸ¦˜ğŸ¤¶ğŸ¦˜ğŸ¦ŠğŸ¥Œ  ğŸ¥·ğŸ¦ŠğŸ§›
  ğŸ§¯ğŸ˜’ğŸ¤«  ğŸ˜¼ğŸ¥ŒğŸ§›ğŸ¥¾ğŸ˜¼ğŸ˜ğŸ¤¶  ğŸ§ğŸ¤«ğŸ§›ğŸ¥¾ğŸ¤«  ğŸ¦ŠğŸ¦ŸğŸ¦˜ğŸ¤¶ğŸ¥ŒğŸ˜¼
  ğŸ§›ğŸ¦ŠğŸ¤¯ğŸ¥¾  ğŸ¥‘ğŸ¦ŸğŸ¥‘ğŸ¤‡ğŸ˜¼  ğŸ˜’ğŸ¦Ÿ  ğŸ¦˜ğŸ§›ğŸ˜  ğŸ¤ŸğŸ˜¼
  ğŸ˜’ğŸ¥¾ğŸ¤¶ğŸ˜’ğŸ¥¾  ğŸ¤‡ğŸ¦˜  ğŸ¥¾ğŸ˜¼  ğŸ¥¦ğŸ¦ŠğŸ¥ŒğŸ¦˜ğŸ¦Ÿ
  ğŸ¤¶ğŸ¦˜ğŸ¦Ÿ  ğŸ§ğŸ§¯  ğŸ¤«ğŸ¤ŸğŸ˜’ğŸ˜’ğŸ¥¦  ğŸ¥¦ğŸ§ğŸ¥ŒğŸ¦˜ğŸ˜¼ğŸ¤ŸğŸ¤¶
  ğŸ¤«ğŸ¥‘ğŸ¦˜ğŸ¤‡ğŸ¥·ğŸ§  ğŸ¥‘ğŸ§  ğŸ¤¶ğŸ§›ğŸ˜’  ğŸ¥·ğŸ¥¦
  ğŸ§¯ğŸ¦ŠğŸ§›  ğŸ˜¼ğŸ¤¯ğŸ¥‘ğŸ¤Ÿ  ğŸ§¯ğŸ˜¼ğŸ§¯ğŸ¦ŠğŸ˜’ğŸ¥¦  ğŸ¥¾ğŸ˜’ğŸ¦˜ğŸ¤«ğŸ¦ºğŸ¤ŸğŸ§¯  ğŸ§›ğŸ˜ğŸ§›ğŸ§›ğŸ˜¼ğŸ¤¯  ğŸ§ğŸ˜’ğŸ¦ºğŸ¦ºğŸ˜’ğŸ¦º
  ğŸ§¯ğŸ˜¼ğŸ¤¯ğŸ§ğŸ¥ŒğŸ¤‡ğŸ§›ğŸ¦˜  ğŸ§¯ğŸ¤¯  ğŸ˜’ğŸ¤ŸğŸ˜¼ğŸ¥¾ğŸ¤ŸğŸ¤«  ğŸ˜ğŸ¤«

Zusammen mit ihrem Kollegen Ã¼berlegen Sie sich ein interessantes Ratespiel.
Das Spiele funktioniert so:

* Zu Beginn wÃ¤hlt ihr Kollege einen der fÃ¼nf Texte zufÃ¤llig.
* Er wÃ¤hlt ein zufÃ¤lliges Zeichen aus dem Text und nennt Ihnen dieses dann.
* Am Anfang wechselt er den Text mit einer Wahrscheinlichkeit von 100% und wÃ¤hlt einen der vier anderen zufÃ¤llig aus. 
* Nach jedem Zeichen reduziert sich diese Wechsel-Wahrscheinlichkeit jedoch um einen Prozentpunkt (99%, 98%, 97%, etc...)
* Er wiederholt Schritt 2 und 3 nun 100 mal bis die Wechsel-Wahrscheinlichkeit bei 0% angekommen ist.
* Sie sollen nun erraten welchen Text der Kollege am Ende in der Hand halten.

Ihr Kollege nennt Ihnen die folgende Zeichenkette

.. code-block:: bash

  ğŸ˜ğŸ˜ğŸ¥¦ğŸ¦ŠğŸ¦ºğŸ¥‘ğŸ¤‡ğŸ§›ğŸ¥¦ğŸ¦ŸğŸ¦˜ğŸ˜¼ğŸ¥¾
  ğŸ¥¦ğŸ¤‡ğŸ¥ŒğŸ¦ºğŸ¤¶ğŸ¦ŠğŸ˜ğŸ¦ŸğŸ¥·ğŸ¥·ğŸ¥ŒğŸ˜’ğŸ¥‘
  ğŸ¦ŸğŸ¦ºğŸ¤¶ğŸ¤¶ğŸ¥¾ğŸ¥¾ğŸ˜¼ğŸ¥‘ğŸ˜ğŸ¤«ğŸ˜ğŸ¦˜
  ğŸ¥·ğŸ¦˜ğŸ¤¯ğŸ¤¯ğŸ¦ŸğŸ¤ŸğŸ¤¯ğŸ˜ğŸ¥·ğŸ¦ŠğŸ¥¾ğŸ¦Ÿ

**Aufgabe**
Schreiben Sie ein Python-Skript welches die Texte sowie die Zeichenkette einlieÃŸt und
mit Hilfe des VorwÃ¤rts-Algorithmus aus der Vorlesung die gesuchte Wahrscheinlichkeit fÃ¼r
jeden der fÃ¼nf Texte berechnet.

Hidden Markov Modelle (HMMs)
----------------------------

Ein Hidden Markov Modell (HMM) ist ein statistisches Modell, das eine Folge von Beobachtungen beschreibt, 
die durch eine zugrunde liegende, **versteckte Zustandsfolge** erzeugt wird. Es eignet sich besonders 
gut fÃ¼r Aufgaben, bei denen man aus einer beobachtbaren Datenreihe (z.B. Zeichen, GerÃ¤usche, etc.) 
auf eine nicht direkt sichtbare Abfolge von ZustÃ¤nden schlieÃŸen mÃ¶chte.

In dieser Aufgabe wird eine Zeichenfolge vorgelesen, bei der die Zeichen **zufÃ¤llig** (aber nicht vÃ¶llig beliebig) aus einem 
bestimmten Ursprungstext stammen. 
Sie sollen mit Hilfe eines HMMs **rekonstruieren**, aus welchem Text diese Zeichen stammen kÃ¶nnten.

Dabei beonachten wir, dass:

* Die Ursprungstexte eine **Verteilung Ã¼ber Buchstaben** aufweist. Beachten Sie das die Zeichen in den fÃ¼nf Texten unterschiedlich hÃ¤ufig vorkommen!

Das Hidden Markov Modell bildet nun diese Annahmen ab:

* Die **ZustÃ¤nde** im Modell entsprechen hypothetisch dem "echten" Text aus dem gerade vorgelesen wird (diese sind **nicht beobachtbar**).
* Die **Beobachtungen** sind die tatsÃ¤chlich gehÃ¶rten Zeichen.
* Die **Ãœbergangswahrscheinlichkeiten** modellieren, wie wahrscheinlich ein Wechsel von einem Text zum nÃ¤chsten ist.
* Die **Emissionswahrscheinlichkeiten** beschreiben, wie wahrscheinlich ein bestimmter Buchstabe vorgelesen wird, gegeben den jeweiligen Text (**bedingte Wahrscheinlichkeit**).

Der VorwÃ¤rts-Algorithmus
------------------------

Der VorwÃ¤rts-Algorithmus ist nun ein Algorithmus zur rekursiven Berechnung einer Wahrscheinlichkeit 
dafÃ¼r sich in einem bestimmten Zustand :math:`x_t` zu befinden gegeben eine Zeitreihe von Beobachtungen 
:math:`y_{1:t}`. Mit der Notation :math:`y_{1:t\}` ist dabei die Menge alle Beobachtungen 
:math:`y_1, y_2, \dots, y_t` gemeint.

Wir wollen nun berechnen

.. math::
  P\left(x_t | y_{1:t}\right)

also die bedingte Wahrscheinlichkeit fÃ¼r einen bestimmten Zustand :math:`x_t` gegeben die Beobachtungen. Wir 
betrachten dazu zunÃ¤chst die Verbundwahrscheinlichkeit :math:`P(x_t, y_{1:t})` und schreiben mit Hilfe des 
`Satzes der totalen Wahrscheinlichkeit <https://en.wikipedia.org/wiki/Law_of_total_probability>`_ 

.. math::
  \alpha_t(x_t) = P(x_t, y_{1:t}) = \sum_{x_{t-1}} p(x_t, x_{t-1}, y_{1:t})

Dabei iteriert die Summe Ã¼ber alle mÃ¶glichen VorgÃ¤ngerzustÃ¤nde. Nach der Definition der 
`bedingten Wahrscheinlichkeit <https://en.wikipedia.org/wiki/Conditional_probability#:~:text=In%20probability%20theory%2C%20conditional%20probability,relationship%20with%20another%20event%20B.>`_ 
schreiben wir dann weiter

.. math::
  \alpha_t(x_t) = \sum_{x_{t-1}} 
  p(y_t | x_t, x_{t-1}, y_{1:t-1})
  p(x_t | x_{t-1}, y_{1:t-1})
  p(x_{t-1}, y_{1:t-1})

Nun kÃ¶nnen wir argumentieren das :math:`y_t` nur von :math:`x_t` abhÃ¤ngt (die Beobachtung zum Zeitpunkt :math:`t` wird 
nur durch den Zustand in diesem Zeitpunkt beeinflusst). Ausserdem hÃ¤ngt 
:math:`x_t` nur von :math:`x_{t-1}` ab (der Folgezustand hÃ¤ngt nur vom VorgÃ¤ngerzustand ab). Damit kÃ¶nnen wir verkÃ¼rzt schreiben

.. math::
  \alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}} 
  p(x_t | x_{t-1})
  \alpha_t(x_{t-1})

Dabei beschreibt :math:`p(y_t | x_t)` die Wahrscheinlichkeit dafÃ¼r in einem konkreten Zustand eine bestimmte 
Beobachtung zu machen. Im Kontext der Aufgabe beschreibt dies also die Wahrscheinlichkeit dafÃ¼r 
ein bestimmtes Symbol aus dem Text zu hÃ¶ren wenn aus einem konkreten (bekannten) Text vorgelesen wird. 

Der Term :math:`p(x_t | x_{t-1})` beschreibt die Wahrscheinlichkeit fÃ¼r einen Ãœbergang von einem Zustand in den nÃ¤chsten.
Im Kontext der Aufgabe also die Wahrscheinlichkeit dafÃ¼r das der Text gewechselt wird bzw. beibehalten wird. 

Der VorwÃ¤rts-Algorithmus funktioniert nur so das zunÃ¤chst die Ausgangswahrscheinlichkeiten
:math:`\alpha_0(x_0)` initialisiert werden. Dann wird fÃ¼r jede Beobachtung nacheinander, also fÃ¼r :math:`t=1,\dots,T` berechnet

.. math::
  \alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}} 
  p(x_t | x_{t-1})
  \alpha_t(x_{t-1})

Die gesuchte Wahrscheinlichkeit fÃ¼r einen konkreten Zustand gegeben die Beobachtungsreihe lautet dann

.. math::
  P(x_T|1_{1:T}) = \frac{\alpha_T(x_T)}{\sum_{x_t} \alpha_T(x_t)}

Man normiert also die Alpha-Werte indem man durch deren Summe Ã¼ber alle mÃ¶glichen ZustÃ¤nde dividiert.

Der Code
--------

In diesem Praktikum arbeiten Sie in der Datei 

.. code-block:: Python
    
    forward.py

Diese ist insofern schon vorbereitet als das die Texte sowie die beobachtete Sequenz an Zeichen 
als Variablen bereits Ã¼bernommen wurden. Wir werden nun ein Hidden Markov Modell 
mit fÃ¼nf ZustÃ¤nden (der jeweils vorgelesene Text) definieren und die Beobachtungswahrscheinlichkeiten
fÃ¼r jedes Zeichen in jedem Zustand bestimmen. AnschlieÃŸend implementieren wir den VorwÃ¤rts-Algorithmus 
und bestimmen mit diesem die Wahrscheinlichkeiten fÃ¼r jeden der fÃ¼nf ZustÃ¤nde gegeben die 
beobachtete Zeichensequenz. 

**Schritt 1**: Den Text sÃ¤ubern
-------------------------------

Bevor wir die Beobachtungswahrscheinlichkeiten bestimmen kÃ¶nnen mÃ¼ssen wir die
Texte zunÃ¤chste von unerwÃ¼nschten Zeichen sÃ¤ubern. Implementieren Sie die Methode

.. autofunction:: forward.clean_text

Verwenden Sie 
`replace <https://www.w3schools.com/python/ref_string_replace.asp>`_ um unerwÃ¼nschte Zeichen zu entfernen.

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python

        def clean_text(text):
          return text.replace(" ", "").replace("\n", "")

**Schritt 2**: Beobachtungswahrscheinlichkeiten pro Text
--------------------------------------------------------

Um die Beobachtungswahrscheinlichkeiten der Zeichen fÃ¼r einen einzelnen Text zu berechnen mÃ¼ssen wir im Grunde
nur zÃ¤hlen wie oft ein bestimmtes Zeichen in diesem Text vorkommt und dies ins VerhÃ¤ltniss zu allen Zeichen in dem Text setzen. 

Implementieren Sie die nun die Methode

.. autofunction:: forward.character_propabilities

indem Sie `count <https://www.w3schools.com/python/ref_string_count.asp>`_ verwenden um die HÃ¤ufigkeit einzelner 
Zeichen in einem String zu zÃ¤hlen. Rufen Sie zunÃ¤chst :py:func:`clean_text` auf um den Ã¼bergebenen Text zu sÃ¤ubern.

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python
        
        def character_propabilities(text, all_chars):
          text = clean_text(text)
          return {
            char: text.count(char) / len(text) for char in all_chars
          }

**Schritt 3**: Alle Beobachtungswahrscheinlichkeiten berechnen
--------------------------------------------------------------

Nun mÃ¼ssen wir lediglich noch einmal systematisch alle Beobachtungswahrscheinlichkeiten
berechnen und als Liste zurÃ¼ckgeben. Implementieren Sie dazu die Methode

.. autofunction:: forward.get_emmision_propabilities

indem Sie den TODO-Anweisungen innerhalb der Methode folgen. Verwenden Sie 
`join <https://www.w3schools.com/python/ref_string_join.asp>`_.

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python
        
        def get_emmision_propabilities(all_texts):
          # Join all texts and clean them
          joined_text = clean_text("".join(all_texts))
          
          # Get a unique list of all characters across all five texts
          all_chars = set(joined_text)

          # Now get the character emmision propabilities for each text
          return [character_propabilities(text, all_chars) for text in all_texts]

**Schritt 4**: Der initiale Alpha-Vektor
----------------------------------------

Wir werden den VorwÃ¤rts-Algorithmus in vektorisierter Form implementieren, d.h. wir
berechnen die geschÃ¤tzten Zustandswahrscheinlichkeiten (die Alpha-Werte :math:`\alpha_t`) als 
`np.array <https://numpy.org/doc/stable/reference/generated/numpy.array.html>`_. Da unser 
Hidden Markov Model fÃ¼nf diskrete ZustÃ¤nde verwaltet (die fÃ¼nf Texte aus denen vorgelesen werden kann) 
ist dieser Vektor fÃ¼nf-dimensional. Um den rekursiven Algorithmus zu starten benÃ¶tigen wir initiale
Werte fÃ¼r ebendiese Alpha-Werte. In unserem konkreten Kontext wissen wir nicht mit welchem Text
der Kollege zu lesen beginnt, die ZustÃ¤nde :math:`x_1, \dots, x_5` sind also alle gleichwahrscheinlich, d.h. 

.. math:: 
    \boldsymbol{\alpha}_0 = (1, 1, 1, 1, 1)

Implementieren Sie nun die Methode

.. autofunction:: forward.get_initial_alpha

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python
        
        def get_initial_alpha():
          # In the begining, we donÂ´t know which text our colleague choose
          # to start with, so all texts are equally likely
          return np.array([1.0, 1.0, 1.0, 1.0, 1.0])


**Schritt 5**: Die ZustandsÃ¼bergangsmatrix
------------------------------------------

Im Laufe des VorwÃ¤rts-Algorthmus mÃ¼ssen wir den Term

.. math::
  \sum_{x_{t-1}}  p(x_t | x_{t-1}) \alpha_t(x_{t-1})

berechnen. Dabei summiert die Summe Ã¼ber alle mÃ¶glichen ZustÃ¤nde :math:`x_{t-1}`, in unserem Fall also
alle fÃ¼nf Text. Die Ãœbergangswahrscheinlichkeiten sind dabei derart das mit 90% Wahrscheinlichkeit der 
selbe Text wieder gewÃ¤hlt wird wÃ¤hrend die restlichen 10% gleichmÃ¤ÃŸig auf die vier verbleibenden Text 
aufgeteilt werden. FÃ¼r z.B. :math:`x_t = 1`, also den ersten Text lÃ¤ÃŸt sich die Summe als Skalarprodukt 

.. math::

    \alpha_t(1) = (0.9, 0.025, 0.025, 0.025, 0.025)\cdot \boldsymbol{\alpha_{t-1}}

und entsprechend

.. math::

    \alpha_t(2) = (0.025, 0.9, 0.025, 0.025, 0.025)\cdot \boldsymbol{\alpha_{t-1}}

etc. Der ZustandsÃ¼bergang vom alten Alpha-Vektor :math:`\boldsymbol{\alpha}_{t-1}` zum neuen Alpha-Vektor :math:`\boldsymbol{\alpha}_t`
lÃ¤ÃŸt sich demnach als Matrixmultiplikation ausdrÃ¼cken. 

Implementieren Sie nun die Methode

.. autofunction:: forward.get_state_transition_matrix 

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python
        
        def get_state_transition_matrix():
          return np.array([
            [0.900, 0.025, 0.025, 0.025, 0.025],
            [0.025, 0.900, 0.025, 0.025, 0.025],
            [0.025, 0.025, 0.900, 0.025, 0.025],
            [0.025, 0.025, 0.025, 0.900, 0.025],
            [0.025, 0.025, 0.025, 0.025, 0.900],
            ])

**Schritt 6**: Ein Schritt im VorwÃ¤rts-Algorithmus
--------------------------------------------------

Was nun noch zu tun bleibt ist einen konkreten Schritt im VorswÃ¤rtsalgorithmus auszurechnen, also konkret

.. math::
  \alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}} 
  p(x_t | x_{t-1})
  \alpha_t(x_{t-1})

fÃ¼r alle ZustÃ¤nde :math:`x_t = 1, 2, 3, 4, 5` zu berechnen. FÃ¼r die Summe haben wir bereits argumentiert
das sich diese als Matrixmultiplikation zwischen dem Alpha-Vektor sowie der ZustandsÃ¼bergangsmatrix (siehe oben) darstellen lÃ¤ÃŸt.   

Der Term :math:`p(y_t | x_t)` entspricht nun der Wahrscheinlichkeit fÃ¼r das Auftreten (observierens, beobachten) des Zeichen 
:math:`y_t` wenn wir im Zustand :math:`x_t` sind. Hier konkret also die Wahrscheinlichkeit fÃ¼r ein bestimmtes Zeichen 
gegen den konkreten Text aus dem es stammt. Diese Wahrscheinlichkeiten haben wir in den Methoden 
:py:func:`forward.character_propabilities` und :py:func:`forward.get_emmision_propabilities` bereits berechnet. 

Implementieren Sie nun die Methode

.. autofunction:: forward.forward 

und folgen Sie den TODO-Anweisungen.

.. admonition:: LÃ¶sung anzeigen
   :class: toggle

   .. code-block:: python
        
        def forward(alpha, character, state_transition_matrix, emmision_propabilities):
          # TODO: Implement state transition and update the alpha vector accordingly
          alpha = state_transition_matrix @ alpha

          # TODO: Retrieve symbol emmision propabilties for the given character and update the alpha vector
          Y = np.array([alphabet[character] for alphabet in emmision_propabilities])
          alpha = Y * alpha

          # TODO: Normalize alpha for better visualization (divide by sum)
          alpha /= alpha.sum()

          # TODO: Return alpha
          return alpha

Ergebnisse          
----------

Wenn Sie alles richtig implemeniert haben sehen Sie eine Grafik welche die Verteilung der 
geschÃ¤tzten Zustandswahrscheinlichkeiten (Alpha-Werte) nach jedem einzelnen Zeichen zeigt. 
Dunklere KÃ¤stchen entsprechen dabei hÃ¶heren Wahrscheinlichkeiten fÃ¼r den jeweiligen Zustand (Text).

.. image:: ./alphaovertime.png
  :width: 800px
  :alt: Alpha Ã¼ber Zeitpunkt
  :align: center

Es ist schÃ¶n zu sehen das zunÃ¤chst (fÃ¼r die ersten 25 Schritte) der erste Text am wahrscheinlichsten ist. 
Text 4 ist jedoch Ã¤hnlich wahrscheinlich. Etwa zu Schritt 25 schÃ¤tzt das Modell nun das der *hidden state* 
(also der Text, aus dem vorgelesen wurde) sich zu Text 4 verÃ¤ndert hat weil die Wahrscheinlichkeit fÃ¼r Text 1 
auf nahezu 0 sinkt. Schaut man in die Ã¼bermittelte Sequenz so steht an 25ter Stelle das Symbol ğŸ˜’.
Dieses kommt im vierten Text tatsÃ¤chlich recht hÃ¤ufig vor wÃ¤hrend es im ersten Text nur ein einziges mal auftaucht. 
Diese Beobachtung scheint also ausschlaggebend dafÃ¼r zu sein das das Modell die Zustandswahrscheinlichkeiten
drastisch verÃ¤ndert und sich fÃ¼r Text 4 entscheidet. 

Zwischen Schritt 30 und 40 konvergiert das Modell immer stÃ¤rker zu der Vermutung das nun aus Text 2 gelesen wird. 
Diese Vermutung hÃ¤lt sich auch bis zum Ende der Sequenz so das ihre SchÃ¤tzung in dem Eingangs erwÃ¤hnten Spiel mit ihrem Kollegen 
also wÃ¤re, das dieser am Ende aus dem zweiten Text liest. Der VorwÃ¤rts-Algorithmus erlaubt es die ZustandsÃ¼bergÃ¤nge 
und die Beobachtung so miteinander zu kombinieren das wir den wahrscheinlichsten Zustand ermitteln kÃ¶nnen. 

MusterlÃ¶sung
------------

:doc:`source`
