

<!DOCTYPE html>
<html class="writer-html5" lang="de" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Der VorwÃ¤rts-Algorithmus &mdash; Machine Perception and Tracking - Praktikum  Dokumentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />

  
    <link rel="canonical" href="https://dmu1981.github.io/MPTPraktikum/forwardalgorithm/index.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=245627df"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script src="../_static/translations.js?v=79cc9f76"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="../genindex.html" />
    <link rel="search" title="Suche" href="../search.html" />
    <link rel="prev" title="Minimum Variance Fusion" href="../multivariate/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Perception and Tracking - Praktikum
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Dokumentation durchsuchen" aria-label="Dokumentation durchsuchen" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Aufgaben:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../webcam/index.html">Die Webcam Ã¶ffnen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kanten/index.html">Kantendetektion mit Sobel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../harris/index.html">Der Harris Eckendetektor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../YOLO/index.html">Objekterkennung mit YOLO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AdaBoost/index.html">AdaBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../HOG/index.html">Histogram of Oriented Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../homogen/index.html">Rechnen mit homogene Koordinaten</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mahalanobis/index.html">Mahalanobis-Distanz und Kovarianzellipsen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mahalanobis/index.html#kovarianzellipsen">Kovarianzellipsen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mahalanobis/index.html#darstellung-von-kovarianzellipsen">Darstellung von Kovarianzellipsen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mahalanobis/index.html#aufgabe-1-zeichnen-einer-kovarianzellipse"><strong>Aufgabe 1</strong>: Zeichnen einer Kovarianzellipse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mahalanobis/index.html#musterlosung">MusterlÃ¶sung</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nonlinearmapping/index.html">Nichtlineare Abbildung normalverteilter Zufallsvariablen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate/index.html">Minimum Varianz Fusion</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Der VorwÃ¤rts-Algorithmus</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#archaologie-das-setting">ArchÃ¤ologie - Das Setting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hidden-markov-modelle-hmms">Hidden Markov Modelle (HMMs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Der VorwÃ¤rts-Algorithmus</a></li>
<li class="toctree-l2"><a class="reference internal" href="#der-code">Der Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-1-den-text-saubern"><strong>Schritt 1</strong>: Den Text sÃ¤ubern</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.clean_text"><code class="docutils literal notranslate"><span class="pre">clean_text()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-2-beobachtungswahrscheinlichkeiten-pro-text"><strong>Schritt 2</strong>: Beobachtungswahrscheinlichkeiten pro Text</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.character_propabilities"><code class="docutils literal notranslate"><span class="pre">character_propabilities()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-3-alle-beobachtungswahrscheinlichkeiten-berechnen"><strong>Schritt 3</strong>: Alle Beobachtungswahrscheinlichkeiten berechnen</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.get_emmision_propabilities"><code class="docutils literal notranslate"><span class="pre">get_emmision_propabilities()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-4-der-initiale-alpha-vektor"><strong>Schritt 4</strong>: Der initiale Alpha-Vektor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.get_initial_alpha"><code class="docutils literal notranslate"><span class="pre">get_initial_alpha()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-5-die-zustandsubergangsmatrix"><strong>Schritt 5</strong>: Die ZustandsÃ¼bergangsmatrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.get_state_transition_matrix"><code class="docutils literal notranslate"><span class="pre">get_state_transition_matrix()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schritt-6-ein-schritt-im-vorwarts-algorithmus"><strong>Schritt 6</strong>: Ein Schritt im VorwÃ¤rts-Algorithmus</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#forward.forward"><code class="docutils literal notranslate"><span class="pre">forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ergebnisse">Ergebnisse</a></li>
<li class="toctree-l2"><a class="reference internal" href="#musterlosung">MusterlÃ¶sung</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Perception and Tracking - Praktikum</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Der VorwÃ¤rts-Algorithmus</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/forwardalgorithm/index.rst.txt" rel="nofollow"> Quelltext anzeigen</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="der-vorwarts-algorithmus">
<h1>Der VorwÃ¤rts-Algorithmus<a class="headerlink" href="#der-vorwarts-algorithmus" title="Link to this heading">ïƒ</a></h1>
<p>In diesem Praktikum implementieren Sie den VorwÃ¤rts-Algorithmus am Beispiel
von Ã¼berlieferten Texten einer fiktiven uralten Zivilisation.</p>
<section id="archaologie-das-setting">
<h2>ArchÃ¤ologie - Das Setting<a class="headerlink" href="#archaologie-das-setting" title="Link to this heading">ïƒ</a></h2>
<p>Bei einer Ausgrabung finden Sie fÃ¼nf Texte einer uralten Zivilisation.
Diese Texte lauten wir folgt</p>
<p><strong>Erster Text</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ§ğŸ§¯<span class="w">  </span>ğŸ¤‡ğŸ¥ŒğŸ¦Ÿ<span class="w">  </span>ğŸ¤‡ğŸ¤«ğŸ¥ŒğŸ˜¼ğŸ¤«<span class="w">  </span>ğŸ¤‡ğŸ¤Ÿ<span class="w">  </span>ğŸ¦˜ğŸ¦ŠğŸ¥¾
ğŸ¤¯ğŸ¤¯ğŸ§›ğŸ˜ğŸ˜¼<span class="w">  </span>ğŸ¥ŒğŸ¥¦ğŸ¤‡ğŸ¤‡ğŸ¦ŸğŸ˜’<span class="w">  </span>ğŸ§ğŸ§›ğŸ§ğŸ¦ŠğŸ§¯<span class="w">  </span>ğŸ¦˜ğŸ˜ğŸ¦˜ğŸ¦ŸğŸ¤¯ğŸ¦º<span class="w">  </span>ğŸ¥¾ğŸ¥¾ğŸ¥‘ğŸ¥‘ğŸ¥Œ
ğŸ¤ŸğŸ§ğŸ¥¦<span class="w">  </span>ğŸ¥¦ğŸ˜¼ğŸ¦ŠğŸ¥¾ğŸ¦ºğŸ¤‡<span class="w">  </span>ğŸ§¯ğŸ¦ŸğŸ¦˜ğŸ¤¶ğŸ¦ºğŸ¦ŸğŸ¥·ğŸ§¯<span class="w">  </span>ğŸ˜ğŸ¦ŸğŸ¦ŠğŸ¤Ÿ
ğŸ§ğŸ˜ğŸ¦ºğŸ¦ŠğŸ¤«ğŸ¥ŒğŸ§¯<span class="w">  </span>ğŸ¦ºğŸ˜<span class="w">  </span>ğŸ¦˜ğŸ˜¼ğŸ¥¦ğŸ˜ğŸ¥‘<span class="w">  </span>ğŸ¤¯ğŸ¦˜ğŸ¦˜ğŸ¦ºğŸ¦º<span class="w">  </span>ğŸ˜¼ğŸ¤ŸğŸ¥¾ğŸ¤¯ğŸ¥¾ğŸ¤¶ğŸ¥¦ğŸ¦˜
ğŸ¦ŸğŸ˜ğŸ¤ŸğŸ¦ºğŸ˜¼ğŸ¤¶ğŸ¤Ÿ<span class="w">  </span>ğŸ§ğŸ¦ŸğŸ˜ğŸ¤¯ğŸ§›ğŸ¤‡ğŸ¤Ÿ<span class="w">  </span>ğŸ§¯ğŸ¤Ÿ<span class="w">  </span>ğŸ˜ğŸ¦˜ğŸ¥ŒğŸ§ğŸ¦˜ğŸ¤¶ğŸ¦ºğŸ¦Š<span class="w">  </span>ğŸ¤¶ğŸ¤¶ğŸ¤«ğŸ¤ŸğŸ¦º
ğŸ¤‡ğŸ¥ŒğŸ˜ğŸ¥ŒğŸ¤ŸğŸ¤‡<span class="w">  </span>ğŸ¥¦ğŸ¦ºğŸ¦˜ğŸ¤‡ğŸ¥¾ğŸ¥·<span class="w">  </span>ğŸ¤¯ğŸ¦˜ğŸ¦ºğŸ˜’<span class="w">  </span>ğŸ§¯ğŸ¦ŸğŸ¦Š<span class="w">  </span>ğŸ¤‡ğŸ§¯ğŸ¥Œ<span class="w">  </span>ğŸ¦ŸğŸ¤‡ğŸ§›ğŸ§›ğŸ¤ŸğŸ¤¯ğŸ§›
ğŸ¤«ğŸ¦Ÿ<span class="w">  </span>ğŸ¥¦ğŸ¤¶ğŸ¥‘ğŸ¤‡ğŸ¥‘<span class="w">  </span>ğŸ¥¦ğŸ¦ŠğŸ¥¦<span class="w">  </span>ğŸ˜¼ğŸ¤‡ğŸ¦ŸğŸ˜ğŸ¥‘<span class="w">  </span>ğŸ§ğŸ¦ŠğŸ¤¯ğŸ¤«ğŸ¦˜ğŸ¥¾ğŸ¦Š
ğŸ˜ğŸ˜¼<span class="w">  </span>ğŸ§›ğŸ˜ğŸ§›ğŸ¦ŠğŸ¤‡ğŸ¦ŠğŸ¦ºğŸ¥·<span class="w">  </span>ğŸ¤¯ğŸ¦˜ğŸ¤¶ğŸ§ğŸ§ğŸ§ğŸ¥Œ<span class="w">  </span>ğŸ¥·ğŸ¤¶ğŸ¦ŠğŸ¦ŠğŸ˜’ğŸ§›<span class="w">  </span>ğŸ˜¼ğŸ˜ğŸ¤«ğŸ§
</pre></div>
</div>
<p><strong>Zweiter Text</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ¦ºğŸ¤¶ğŸ¦ºğŸ¦Ÿ<span class="w">  </span>ğŸ¦˜ğŸ§¯<span class="w">  </span>ğŸ§›ğŸ¥‘ğŸ¤¯<span class="w">  </span>ğŸ¤¶ğŸ¥ŒğŸ¤‡ğŸ¥·ğŸ˜’
ğŸ§ğŸ¦˜ğŸ¥¾<span class="w">  </span>ğŸ˜ğŸ¤¯<span class="w">  </span>ğŸ¤«ğŸ§ğŸ¤ŸğŸ¦ºğŸ¤ŸğŸ§¯<span class="w">  </span>ğŸ¦ºğŸ¤ŸğŸ¦ŸğŸ¤«ğŸ˜¼
ğŸ¤¶ğŸ˜’ğŸ¦ºğŸ¥¾<span class="w">  </span>ğŸ§ğŸ¤¶ğŸ¥·ğŸ§›ğŸ§›ğŸ¤«ğŸ§›<span class="w">  </span>ğŸ¤‡ğŸ¥¾ğŸ¤«ğŸ¤‡ğŸ˜’ğŸ¦ŸğŸ¦Ÿ<span class="w">  </span>ğŸ¤‡ğŸ¥¦ğŸ¥·ğŸ§ğŸ¥‘
ğŸ˜ğŸ§¯ğŸ¦˜ğŸ¦Ÿ<span class="w">  </span>ğŸ§›ğŸ¦ŸğŸ¤‡ğŸ§›ğŸ¤«<span class="w">  </span>ğŸ˜¼ğŸ¤ŸğŸ¦˜<span class="w">  </span>ğŸ˜ğŸ¦ŸğŸ¤¯
ğŸ¥·ğŸ¤¯ğŸ¤ŸğŸ§¯ğŸ§<span class="w">  </span>ğŸ§ğŸ¤¶ğŸ§ğŸ§›ğŸ¦˜<span class="w">  </span>ğŸ§ğŸ¤¶ğŸ§¯ğŸ¥‘ğŸ¤¯ğŸ˜’<span class="w">  </span>ğŸ¥¦ğŸ¥¾ğŸ¤¶ğŸ¤«ğŸ˜<span class="w">  </span>ğŸ˜ğŸ˜ğŸ¤¯ğŸ˜¼ğŸ¤¶ğŸ§ğŸ¥·ğŸ¦Ÿ
ğŸ§›ğŸ˜¼ğŸ¤Ÿ<span class="w">  </span>ğŸ¥‘ğŸ¤‡ğŸ¥¾<span class="w">  </span>ğŸ¥¾ğŸ˜’ğŸ¤«<span class="w">  </span>ğŸ§ğŸ˜ğŸ§›ğŸ¤ŸğŸ¦ºğŸ¥‘ğŸ¤¯ğŸ¤‡<span class="w">  </span>ğŸ¦ŸğŸ§¯ğŸ¥·<span class="w">  </span>ğŸ¥¾ğŸ¤‡ğŸ¥¾
ğŸ¥¦ğŸ§ğŸ¦ŠğŸ˜ğŸ§ğŸ˜’<span class="w">  </span>ğŸ˜ğŸ¤‡ğŸ˜¼ğŸ¤¯ğŸ¦ŠğŸ¥¾ğŸ¥·<span class="w">  </span>ğŸ˜ğŸ¥ŒğŸ¤¯ğŸ¦ŸğŸ§›ğŸ§ğŸ§›<span class="w">  </span>ğŸ¦˜ğŸ§<span class="w">  </span>ğŸ˜ğŸ˜’ğŸ¦ŠğŸ§›<span class="w">  </span>ğŸ¥¾ğŸ¥¾ğŸ¤¶ğŸ¤¯
ğŸ˜ğŸ˜’ğŸ§›ğŸ¤¯ğŸ§<span class="w">  </span>ğŸ¤ŸğŸ¦ºğŸ¤¯ğŸ§›<span class="w">  </span>ğŸ¦ŠğŸ¤¶ğŸ¥¾ğŸ¥‘ğŸ¥·ğŸ¦˜ğŸ¦º<span class="w">  </span>ğŸ¦ŸğŸ˜¼ğŸ¦˜<span class="w">  </span>ğŸ¦˜ğŸ¤¶ğŸ˜¼ğŸ¤«<span class="w">  </span>ğŸ¦ŸğŸ¦˜ğŸ˜¼ğŸ¤¯ğŸ¤¯
</pre></div>
</div>
<p><strong>Dritter Text</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ§¯ğŸ§›ğŸ¦ŸğŸ¤«ğŸ¥¦ğŸ¤‡ğŸ¦Ÿ<span class="w">  </span>ğŸ˜ğŸ¥‘ğŸ¤ŸğŸ¥·ğŸ§¯ğŸ¤«ğŸ¤«<span class="w">  </span>ğŸ¥·ğŸ§¯ğŸ¦ºğŸ¦º<span class="w">  </span>ğŸ§›ğŸ¥‘ğŸ¥¾ğŸ¦ºğŸ§
ğŸ¤‡ğŸ¦ŸğŸ§ğŸ¥ŒğŸ¤‡ğŸ¥¾ğŸ¤¶ğŸ¥‘<span class="w">  </span>ğŸ¥¾ğŸ¤«<span class="w">  </span>ğŸ˜¼ğŸ¥‘ğŸ¦º<span class="w">  </span>ğŸ¤¶ğŸ¦ºğŸ¦Ÿ<span class="w">  </span>ğŸ¤«ğŸ¦º<span class="w">  </span>ğŸ¥¦ğŸ˜¼ğŸ§¯ğŸ¤«ğŸ¦ºğŸ¤¶
ğŸ¤ŸğŸ¥¾ğŸ¦ºğŸ¤‡<span class="w">  </span>ğŸ¤¯ğŸ¥¦ğŸ¤«ğŸ¦ŸğŸ¤¯ğŸ¤ŸğŸ¤ŸğŸ¤¶<span class="w">  </span>ğŸ§›ğŸ¥¦ğŸ§<span class="w">  </span>ğŸ¦˜ğŸ§¯ğŸ¥¾ğŸ§›ğŸ§¯<span class="w">  </span>ğŸ¥‘ğŸ¤¶ğŸ˜¼
ğŸ˜ğŸ˜ğŸ¥¾ğŸ§›ğŸ§›<span class="w">  </span>ğŸ¦˜ğŸ¥‘ğŸ§ğŸ˜¼ğŸ¥ŒğŸ¤«ğŸ¤Ÿ<span class="w">  </span>ğŸ¤ŸğŸ¤‡ğŸ¤¶ğŸ¦ŠğŸ¥¾<span class="w">  </span>ğŸ¤«ğŸ˜¼ğŸ¤«ğŸ¥¾ğŸ¦ŸğŸ§<span class="w">  </span>ğŸ¤‡ğŸ¦˜ğŸ¦ºğŸ§›ğŸ˜¼<span class="w">  </span>ğŸ¥¾ğŸ¤‡ğŸ¥ŒğŸ§ğŸ¤«ğŸ¥·ğŸ¦ŸğŸ¤«
ğŸ¥‘ğŸ§ğŸ§ğŸ¤«ğŸ¥‘ğŸ¦ŸğŸ¤ŸğŸ¥‘<span class="w">  </span>ğŸ§›ğŸ¥ŒğŸ¥¾<span class="w">  </span>ğŸ˜¼ğŸ˜¼ğŸ¤¶ğŸ¤ŸğŸ¦˜<span class="w">  </span>ğŸ¤ŸğŸ¦ºğŸ¤‡ğŸ¦˜ğŸ§›ğŸ¤¯ğŸ¥Œ<span class="w">  </span>ğŸ§ğŸ¦ŸğŸ¤¯ğŸ¥¾
ğŸ¦˜ğŸ¥‘<span class="w">  </span>ğŸ˜¼ğŸ¥¦ğŸ¦ŸğŸ§›ğŸ¥‘ğŸ§¯ğŸ¥Œ<span class="w">  </span>ğŸ¤‡ğŸ˜¼<span class="w">  </span>ğŸ˜¼ğŸ¤«
ğŸ¥¾ğŸ¦ŠğŸ¥·<span class="w">  </span>ğŸ¦˜ğŸ¤Ÿ<span class="w">  </span>ğŸ¦ŠğŸ¥‘ğŸ¤¯ğŸ¤«ğŸ¥ŒğŸ§›<span class="w">  </span>ğŸ¤‡ğŸ¤«ğŸ˜ğŸ¦ºğŸ¥Œ<span class="w">  </span>ğŸ¥¾ğŸ¥¦ğŸ¥‘ğŸ¦ŠğŸ§<span class="w">  </span>ğŸ¥ŒğŸ˜¼ğŸ˜ğŸ¤«ğŸ¤¯
ğŸ¥¦ğŸ¤¶ğŸ§›ğŸ¤ŸğŸ˜’ğŸ¦ŸğŸ§ğŸ§¯<span class="w">  </span>ğŸ¤‡ğŸ¤«ğŸ§›ğŸ¥¦ğŸ¦ºğŸ¤Ÿ<span class="w">  </span>ğŸ¥¦ğŸ¤¶ğŸ¥‘ğŸ¦Ÿ<span class="w">  </span>ğŸ¦ºğŸ§›ğŸ¦˜ğŸ¥¦ğŸ§¯ğŸ¦º
</pre></div>
</div>
<p><strong>Vierter Text</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ˜ğŸ§ğŸ¥¦ğŸ¦ŠğŸ¦ŸğŸ¥¦<span class="w">  </span>ğŸ¤¶ğŸ§¯ğŸ¥ŒğŸ§¯ğŸ¤Ÿ<span class="w">  </span>ğŸ§¯ğŸ¦ºğŸ§¯ğŸ˜’ğŸ§ğŸ§›<span class="w">  </span>ğŸ¦ŠğŸ§ğŸ¥‘ğŸ¤«ğŸ¤¶<span class="w">  </span>ğŸ¤¶ğŸ§ğŸ¦ºğŸ¦ŠğŸ˜ğŸ¤«ğŸ§›ğŸ¥·<span class="w">  </span>ğŸ˜’ğŸ¤‡ğŸ¥ŒğŸ¤¯ğŸ¤‡ğŸ¤¶ğŸ¤¶ğŸ¥¾
ğŸ¦ŠğŸ˜¼ğŸ¦ŸğŸ¦˜ğŸ˜ğŸ§›ğŸ¤Ÿ<span class="w">  </span>ğŸ˜ğŸ¤‡ğŸ¤¯ğŸ§›ğŸ¤‡ğŸ¦˜ğŸ¦˜ğŸ˜¼<span class="w">  </span>ğŸ¥‘ğŸ¥·ğŸ¥‘ğŸ¤‡ğŸ¤ŸğŸ¤‡<span class="w">  </span>ğŸ¦ŠğŸ¥‘ğŸ¥ŒğŸ˜¼ğŸ¦˜ğŸ¤‡<span class="w">  </span>ğŸ˜¼ğŸ¤Ÿ<span class="w">  </span>ğŸ¦ºğŸ¦ŸğŸ¥·ğŸ˜’ğŸ¥¦ğŸ¤¯
ğŸ¤‡ğŸ¦Ÿ<span class="w">  </span>ğŸ§›ğŸ¤ŸğŸ¥¾ğŸ¤¯ğŸ¥¦<span class="w">  </span>ğŸ¥¦ğŸ˜’ğŸ˜ğŸ¥‘ğŸ¥·ğŸ¤ŸğŸ¥¾<span class="w">  </span>ğŸ¥‘ğŸ¤¶<span class="w">  </span>ğŸ¦ºğŸ¤«ğŸ¥·ğŸ˜’ğŸ¦ºğŸ¤Ÿ<span class="w">  </span>ğŸ¥¦ğŸ˜’ğŸ§›ğŸ§¯ğŸ¤‡ğŸ¥‘ğŸ¤¶ğŸ¥¾
ğŸ˜ğŸ¤ŸğŸ¦ºğŸ˜’ğŸ¥¾ğŸ¤‡ğŸ¤¯ğŸ¤¶<span class="w">  </span>ğŸ¥¦ğŸ¦ŸğŸ¥¦ğŸ¤ŸğŸ¤«ğŸ¤¯ğŸ¥‘ğŸ¤‡<span class="w">  </span>ğŸ¤ŸğŸ¦ŠğŸ¥¾ğŸ¤ŸğŸ¤¶ğŸ§¯<span class="w">  </span>ğŸ˜¼ğŸ¤¯ğŸ˜’<span class="w">  </span>ğŸ¥¦ğŸ¤‡ğŸ§
ğŸ˜’ğŸ¤ŸğŸ¥ŒğŸ¥ŒğŸ¤«ğŸ§›ğŸ˜¼<span class="w">  </span>ğŸ¤¯ğŸ¦ŸğŸ¥‘ğŸ¤‡ğŸ¥‘<span class="w">  </span>ğŸ¥‘ğŸ¦ŸğŸ˜ğŸ˜¼ğŸ˜<span class="w">  </span>ğŸ¦˜ğŸ¦ŸğŸ¥¦ğŸ¤‡ğŸ¦˜
ğŸ¤¯ğŸ˜ğŸ¥¦ğŸ¦ŠğŸ˜¼ğŸ˜ğŸ¤¶ğŸ¤‡<span class="w">  </span>ğŸ˜¼ğŸ¤¯ğŸ¤¯ğŸ˜ğŸ¥¾ğŸ˜¼ğŸ¥·<span class="w">  </span>ğŸ¤¯ğŸ¥‘ğŸ¦ŸğŸ˜¼<span class="w">  </span>ğŸ¥·ğŸ˜ğŸ¤‡ğŸ¥Œ<span class="w">  </span>ğŸ¦ºğŸ¦ºğŸ¦ºğŸ¥ŒğŸ¦˜ğŸ§›ğŸ¤«ğŸ¤‡
ğŸ¥·ğŸ§ğŸ¥¦ğŸ˜<span class="w">  </span>ğŸ§ğŸ¤‡ğŸ˜’ğŸ¦ŠğŸ¤¯ğŸ¤¯ğŸ˜ğŸ˜¼<span class="w">  </span>ğŸ§¯ğŸ§ğŸ¥¦<span class="w">  </span>ğŸ˜ğŸ¤¯<span class="w">  </span>ğŸ¦ŸğŸ¤«ğŸ¥‘ğŸ¤ŸğŸ¤«ğŸ¦Ÿ<span class="w">  </span>ğŸ˜¼ğŸ§ğŸ¦ºğŸ˜’ğŸ˜’ğŸ¦ºğŸ¤«ğŸ¥¦
ğŸ§›ğŸ¥¦ğŸ¦ŸğŸ˜¼ğŸ˜’ğŸ˜’ğŸ¦˜<span class="w">  </span>ğŸ¤‡ğŸ¥¾<span class="w">  </span>ğŸ¥ŒğŸ¥¦ğŸ§›ğŸ¤¯ğŸ¤¶<span class="w">  </span>ğŸ¦ŸğŸ¥·ğŸ¤‡ğŸ¥ŒğŸ˜’ğŸ¤¶<span class="w">  </span>ğŸ¦ŸğŸ¤¯ğŸ˜¼ğŸ˜¼
</pre></div>
</div>
<p><strong>FÃ¼nfter Text</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ¥ŒğŸ¦ŠğŸ¥¾ğŸ¦ŠğŸ§ğŸ§¯ğŸ§›ğŸ¤¯<span class="w">  </span>ğŸ§›ğŸ˜ğŸ¦ŸğŸ¥ŒğŸ¦˜ğŸ¥¾ğŸ¤¯<span class="w">  </span>ğŸ˜ğŸ¤¶ğŸ§ğŸ¥ŒğŸ¦˜ğŸ¤‡<span class="w">  </span>ğŸ§›ğŸ¦ŸğŸ¤¯<span class="w">  </span>ğŸ¦ŠğŸ¦˜ğŸ¤¶ğŸ¦˜ğŸ¦ŠğŸ¥Œ<span class="w">  </span>ğŸ¥·ğŸ¦ŠğŸ§›
ğŸ§¯ğŸ˜’ğŸ¤«<span class="w">  </span>ğŸ˜¼ğŸ¥ŒğŸ§›ğŸ¥¾ğŸ˜¼ğŸ˜ğŸ¤¶<span class="w">  </span>ğŸ§ğŸ¤«ğŸ§›ğŸ¥¾ğŸ¤«<span class="w">  </span>ğŸ¦ŠğŸ¦ŸğŸ¦˜ğŸ¤¶ğŸ¥ŒğŸ˜¼
ğŸ§›ğŸ¦ŠğŸ¤¯ğŸ¥¾<span class="w">  </span>ğŸ¥‘ğŸ¦ŸğŸ¥‘ğŸ¤‡ğŸ˜¼<span class="w">  </span>ğŸ˜’ğŸ¦Ÿ<span class="w">  </span>ğŸ¦˜ğŸ§›ğŸ˜<span class="w">  </span>ğŸ¤ŸğŸ˜¼
ğŸ˜’ğŸ¥¾ğŸ¤¶ğŸ˜’ğŸ¥¾<span class="w">  </span>ğŸ¤‡ğŸ¦˜<span class="w">  </span>ğŸ¥¾ğŸ˜¼<span class="w">  </span>ğŸ¥¦ğŸ¦ŠğŸ¥ŒğŸ¦˜ğŸ¦Ÿ
ğŸ¤¶ğŸ¦˜ğŸ¦Ÿ<span class="w">  </span>ğŸ§ğŸ§¯<span class="w">  </span>ğŸ¤«ğŸ¤ŸğŸ˜’ğŸ˜’ğŸ¥¦<span class="w">  </span>ğŸ¥¦ğŸ§ğŸ¥ŒğŸ¦˜ğŸ˜¼ğŸ¤ŸğŸ¤¶
ğŸ¤«ğŸ¥‘ğŸ¦˜ğŸ¤‡ğŸ¥·ğŸ§<span class="w">  </span>ğŸ¥‘ğŸ§<span class="w">  </span>ğŸ¤¶ğŸ§›ğŸ˜’<span class="w">  </span>ğŸ¥·ğŸ¥¦
ğŸ§¯ğŸ¦ŠğŸ§›<span class="w">  </span>ğŸ˜¼ğŸ¤¯ğŸ¥‘ğŸ¤Ÿ<span class="w">  </span>ğŸ§¯ğŸ˜¼ğŸ§¯ğŸ¦ŠğŸ˜’ğŸ¥¦<span class="w">  </span>ğŸ¥¾ğŸ˜’ğŸ¦˜ğŸ¤«ğŸ¦ºğŸ¤ŸğŸ§¯<span class="w">  </span>ğŸ§›ğŸ˜ğŸ§›ğŸ§›ğŸ˜¼ğŸ¤¯<span class="w">  </span>ğŸ§ğŸ˜’ğŸ¦ºğŸ¦ºğŸ˜’ğŸ¦º
ğŸ§¯ğŸ˜¼ğŸ¤¯ğŸ§ğŸ¥ŒğŸ¤‡ğŸ§›ğŸ¦˜<span class="w">  </span>ğŸ§¯ğŸ¤¯<span class="w">  </span>ğŸ˜’ğŸ¤ŸğŸ˜¼ğŸ¥¾ğŸ¤ŸğŸ¤«<span class="w">  </span>ğŸ˜ğŸ¤«
</pre></div>
</div>
<p>Zusammen mit ihrem Kollegen Ã¼berlegen Sie sich ein interessantes Ratespiel.
Das Spiele funktioniert so:</p>
<ul class="simple">
<li><p>Zu Beginn wÃ¤hlt ihr Kollege einen der fÃ¼nf Texte zufÃ¤llig.</p></li>
<li><p>Er wÃ¤hlt ein zufÃ¤lliges Zeichen aus dem Text und nennt Ihnen dieses dann.</p></li>
<li><p>Nun wechselt er den Text mit einer Wahrscheinlichkeit von 10% und wÃ¤hlt einen der vier anderen zufÃ¤llig aus.</p></li>
<li><p>Er wiederholt Schritt 2 und 3 nun 50 mal und nennt ihnen jeweils ein zufÃ¤lliges Zeichen aus dem Text, den er gerade in der Hand hÃ¤lt.</p></li>
<li><p>Sie sollen nun erraten welchen Text der Kollege am Ende in der Hand halten.</p></li>
</ul>
<p>Ihr Kollege nennt Ihnen die folgende Zeichenkette</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ğŸ˜ğŸ˜ğŸ¥¦ğŸ¦ŠğŸ¦ºğŸ¥‘ğŸ¤‡ğŸ§›ğŸ¥¦ğŸ¦ŸğŸ¦˜ğŸ˜¼ğŸ¥¾
ğŸ¥¦ğŸ¤‡ğŸ¥ŒğŸ¦ºğŸ¤¶ğŸ¦ŠğŸ˜ğŸ¦ŸğŸ¥·ğŸ¥·ğŸ¥ŒğŸ˜’ğŸ¥‘
ğŸ¦ŸğŸ¦ºğŸ¤¶ğŸ¤¶ğŸ¥¾ğŸ¥¾ğŸ˜¼ğŸ¥‘ğŸ˜ğŸ¤«ğŸ˜ğŸ¦˜
ğŸ¥·ğŸ¦˜ğŸ¤¯ğŸ¤¯ğŸ¦ŸğŸ¤ŸğŸ¤¯ğŸ˜ğŸ¥·ğŸ¦ŠğŸ¥¾ğŸ¦Ÿ
</pre></div>
</div>
<p><strong>Aufgabe</strong>
Schreiben Sie ein Python-Skript welches die Texte sowie die Zeichenkette einlieÃŸt und
mit Hilfe des VorwÃ¤rts-Algorithmus aus der Vorlesung die gesuchte Wahrscheinlichkeit fÃ¼r
jeden der fÃ¼nf Texte berechnet.</p>
</section>
<section id="hidden-markov-modelle-hmms">
<h2>Hidden Markov Modelle (HMMs)<a class="headerlink" href="#hidden-markov-modelle-hmms" title="Link to this heading">ïƒ</a></h2>
<p>Ein Hidden Markov Modell (HMM) ist ein statistisches Modell, das eine Folge von Beobachtungen beschreibt,
die durch eine zugrunde liegende, <strong>versteckte Zustandsfolge</strong> erzeugt wird. Es eignet sich besonders
gut fÃ¼r Aufgaben, bei denen man aus einer beobachtbaren Datenreihe (z.B. Zeichen, GerÃ¤usche, etc.)
auf eine nicht direkt sichtbare Abfolge von ZustÃ¤nden schlieÃŸen mÃ¶chte.</p>
<p>In dieser Aufgabe wird eine Zeichenfolge vorgelesen, bei der die Zeichen <strong>zufÃ¤llig</strong> (aber nicht vÃ¶llig beliebig) aus einem
bestimmten Ursprungstext stammen.
Sie sollen mit Hilfe eines HMMs <strong>rekonstruieren</strong>, aus welchem Text diese Zeichen stammen kÃ¶nnten.</p>
<p>Dabei beonachten wir, dass:</p>
<ul class="simple">
<li><p>Die Ursprungstexte eine <strong>Verteilung Ã¼ber Buchstaben</strong> aufweist. Beachten Sie das die Zeichen in den fÃ¼nf Texten unterschiedlich hÃ¤ufig vorkommen!</p></li>
</ul>
<p>Das Hidden Markov Modell bildet nun diese Annahmen ab:</p>
<ul class="simple">
<li><p>Die <strong>ZustÃ¤nde</strong> im Modell entsprechen hypothetisch dem â€echtenâ€œ Text aus dem gerade vorgelesen wird (diese sind <strong>nicht beobachtbar</strong>).</p></li>
<li><p>Die <strong>Beobachtungen</strong> sind die tatsÃ¤chlich gehÃ¶rten Zeichen.</p></li>
<li><p>Die <strong>Ãœbergangswahrscheinlichkeiten</strong> modellieren, wie wahrscheinlich ein Wechsel von einem Text zum nÃ¤chsten ist.</p></li>
<li><p>Die <strong>Emissionswahrscheinlichkeiten</strong> beschreiben, wie wahrscheinlich ein bestimmter Buchstabe vorgelesen wird, gegeben den jeweiligen Text (<strong>bedingte Wahrscheinlichkeit</strong>).</p></li>
</ul>
</section>
<section id="id1">
<h2>Der VorwÃ¤rts-Algorithmus<a class="headerlink" href="#id1" title="Link to this heading">ïƒ</a></h2>
<p>Der VorwÃ¤rts-Algorithmus ist nun ein Algorithmus zur rekursiven Berechnung einer Wahrscheinlichkeit
dafÃ¼r sich in einem bestimmten Zustand <span class="math notranslate nohighlight">\(x_t\)</span> zu befinden gegeben eine Zeitreihe von Beobachtungen
<span class="math notranslate nohighlight">\(y_{1:t}\)</span>. Mit der Notation <span class="math notranslate nohighlight">\(y_{1:t\}\)</span> ist dabei die Menge alle Beobachtungen
<span class="math notranslate nohighlight">\(y_1, y_2, \dots, y_t\)</span> gemeint.</p>
<p>Wir wollen nun berechnen</p>
<div class="math notranslate nohighlight">
\[P\left(x_t | y_{1:t}\right)\]</div>
<p>also die bedingte Wahrscheinlichkeit fÃ¼r einen bestimmten Zustand <span class="math notranslate nohighlight">\(x_t\)</span> gegeben die Beobachtungen. Wir
betrachten dazu zunÃ¤chst die Verbundwahrscheinlichkeit <span class="math notranslate nohighlight">\(P(x_t, y_{1:t})\)</span> und schreiben mit Hilfe des
<a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_total_probability">Satzes der totalen Wahrscheinlichkeit</a></p>
<div class="math notranslate nohighlight">
\[\alpha_t(x_t) = P(x_t, y_{1:t}) = \sum_{x_{t-1}} p(x_t, x_{t-1}, y_{1:t})\]</div>
<p>Dabei iteriert die Summe Ã¼ber alle mÃ¶glichen VorgÃ¤ngerzustÃ¤nde. Nach der Definition der
<a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_probability#:~:text=In%20probability%20theory%2C%20conditional%20probability,relationship%20with%20another%20event%20B.">bedingten Wahrscheinlichkeit</a>
schreiben wir dann weiter</p>
<div class="math notranslate nohighlight">
\[\alpha_t(x_t) = \sum_{x_{t-1}}
p(y_t | x_t, x_{t-1}, y_{1:t-1})
p(x_t | x_{t-1}, y_{1:t-1})
p(x_{t-1}, y_{1:t-1})\]</div>
<p>Nun kÃ¶nnen wir argumentieren das <span class="math notranslate nohighlight">\(y_t\)</span> nur von <span class="math notranslate nohighlight">\(x_t\)</span> abhÃ¤ngt (die Beobachtung zum Zeitpunkt <span class="math notranslate nohighlight">\(t\)</span> wird
nur durch den Zustand in diesem Zeitpunkt beeinflusst). Ausserdem hÃ¤ngt
<span class="math notranslate nohighlight">\(x_t\)</span> nur von <span class="math notranslate nohighlight">\(x_{t-1}\)</span> ab (der Folgezustand hÃ¤ngt nur vom VorgÃ¤ngerzustand ab). Damit kÃ¶nnen wir verkÃ¼rzt schreiben</p>
<div class="math notranslate nohighlight">
\[\alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}}
p(x_t | x_{t-1})
\alpha_t(x_{t-1})\]</div>
<p>Dabei beschreibt <span class="math notranslate nohighlight">\(p(y_t | x_t)\)</span> die Wahrscheinlichkeit dafÃ¼r in einem konkreten Zustand eine bestimmte
Beobachtung zu machen. Im Kontext der Aufgabe beschreibt dies also die Wahrscheinlichkeit dafÃ¼r
ein bestimmtes Symbol aus dem Text zu hÃ¶ren wenn aus einem konkreten (bekannten) Text vorgelesen wird.</p>
<p>Der Term <span class="math notranslate nohighlight">\(p(x_t | x_{t-1})\)</span> beschreibt die Wahrscheinlichkeit fÃ¼r einen Ãœbergang von einem Zustand in den nÃ¤chsten.
Im Kontext der Aufgabe also die Wahrscheinlichkeit dafÃ¼r das der Text gewechselt wird bzw. beibehalten wird.</p>
<p>Der VorwÃ¤rts-Algorithmus funktioniert nur so das zunÃ¤chst die Ausgangswahrscheinlichkeiten
<span class="math notranslate nohighlight">\(\alpha_0(x_0)\)</span> initialisiert werden. Dann wird fÃ¼r jede Beobachtung nacheinander, also fÃ¼r <span class="math notranslate nohighlight">\(t=1,\dots,T\)</span> berechnet</p>
<div class="math notranslate nohighlight">
\[\alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}}
p(x_t | x_{t-1})
\alpha_t(x_{t-1})\]</div>
<p>Die gesuchte Wahrscheinlichkeit fÃ¼r einen konkreten Zustand gegeben die Beobachtungsreihe lautet dann</p>
<div class="math notranslate nohighlight">
\[P(x_T|1_{1:T}) = \frac{\alpha_T(x_T)}{\sum_{x_t} \alpha_T(x_t)}\]</div>
<p>Man normiert also die Alpha-Werte indem man durch deren Summe Ã¼ber alle mÃ¶glichen ZustÃ¤nde dividiert.</p>
</section>
<section id="der-code">
<h2>Der Code<a class="headerlink" href="#der-code" title="Link to this heading">ïƒ</a></h2>
<p>In diesem Praktikum arbeiten Sie in der Datei</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">forward</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Diese ist insofern schon vorbereitet als das die Texte sowie die beobachtete Sequenz an Zeichen
als Variablen bereits Ã¼bernommen wurden. Wir werden nun ein Hidden Markov Modell
mit fÃ¼nf ZustÃ¤nden (der jeweils vorgelesene Text) definieren und die Beobachtungswahrscheinlichkeiten
fÃ¼r jedes Zeichen in jedem Zustand bestimmen. AnschlieÃŸend implementieren wir den VorwÃ¤rts-Algorithmus
und bestimmen mit diesem die Wahrscheinlichkeiten fÃ¼r jeden der fÃ¼nf ZustÃ¤nde gegeben die
beobachtete Zeichensequenz.</p>
</section>
<section id="schritt-1-den-text-saubern">
<h2><strong>Schritt 1</strong>: Den Text sÃ¤ubern<a class="headerlink" href="#schritt-1-den-text-saubern" title="Link to this heading">ïƒ</a></h2>
<p>Bevor wir die Beobachtungswahrscheinlichkeiten bestimmen kÃ¶nnen mÃ¼ssen wir die
Texte zunÃ¤chste von unerwÃ¼nschten Zeichen sÃ¤ubern. Implementieren Sie die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.clean_text">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">clean_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#clean_text"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.clean_text" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>:
Clean the text by removing all white spaces and new line character (\n)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameter<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> â€“ The text to clean</p>
</dd>
<dt class="field-even">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-even"><p>The same text witout white spaces and new line characters</p>
</dd>
</dl>
</dd></dl>

<p>Verwenden Sie
<a class="reference external" href="https://www.w3schools.com/python/ref_string_replace.asp">replace</a> um unerwÃ¼nschte Zeichen zu entfernen.</p>
<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="schritt-2-beobachtungswahrscheinlichkeiten-pro-text">
<h2><strong>Schritt 2</strong>: Beobachtungswahrscheinlichkeiten pro Text<a class="headerlink" href="#schritt-2-beobachtungswahrscheinlichkeiten-pro-text" title="Link to this heading">ïƒ</a></h2>
<p>Um die Beobachtungswahrscheinlichkeiten der Zeichen fÃ¼r einen einzelnen Text zu berechnen mÃ¼ssen wir im Grunde
nur zÃ¤hlen wie oft ein bestimmtes Zeichen in diesem Text vorkommt und dies ins VerhÃ¤ltniss zu allen Zeichen in dem Text setzen.</p>
<p>Implementieren Sie die nun die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.character_propabilities">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">character_propabilities</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_chars</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#character_propabilities"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.character_propabilities" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>:
Given a text, calculate the empirical observation propability of
all characters from the â€all_charsâ€œ list.</p>
<p>The observation propability for character c
is given as the number of occurrences of that character divided by the total
number of characters in the string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameter<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> â€“ The text for which character observation propabilities are to be calculated</p></li>
<li><p><strong>all_chars</strong> â€“ A set of unique characters. The propability for each such character is to be calcualted.</p></li>
</ul>
</dd>
<dt class="field-even">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary mapping all characters within the all_chars parameter to its respective observation propability.</p>
</dd>
</dl>
</dd></dl>

<p>indem Sie <a class="reference external" href="https://www.w3schools.com/python/ref_string_count.asp">count</a> verwenden um die HÃ¤ufigkeit einzelner
Zeichen in einem String zu zÃ¤hlen. Rufen Sie zunÃ¤chst <code class="xref py py-func docutils literal notranslate"><span class="pre">clean_text()</span></code> auf um den Ã¼bergebenen Text zu sÃ¤ubern.</p>
<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">character_propabilities</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">all_chars</span><span class="p">):</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">{</span>
    <span class="n">char</span><span class="p">:</span> <span class="n">text</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">all_chars</span>
  <span class="p">}</span>
</pre></div>
</div>
</div>
</section>
<section id="schritt-3-alle-beobachtungswahrscheinlichkeiten-berechnen">
<h2><strong>Schritt 3</strong>: Alle Beobachtungswahrscheinlichkeiten berechnen<a class="headerlink" href="#schritt-3-alle-beobachtungswahrscheinlichkeiten-berechnen" title="Link to this heading">ïƒ</a></h2>
<p>Nun mÃ¼ssen wir lediglich noch einmal systematisch alle Beobachtungswahrscheinlichkeiten
berechnen und als Liste zurÃ¼ckgeben. Implementieren Sie dazu die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.get_emmision_propabilities">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">get_emmision_propabilities</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all_texts</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#get_emmision_propabilities"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.get_emmision_propabilities" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>:
Return the emmision propabilities for each character in all the sets.
This is essentially a list of dictionaries provided by <a class="reference internal" href="#forward.character_propabilities" title="forward.character_propabilities"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward.character_propabilities()</span></code></a></p>
<ul class="simple">
<li><p>Join all the texts together and clean the result (call <a class="reference internal" href="#forward.clean_text" title="forward.clean_text"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clean_text()</span></code></a>).</p></li>
<li><p>Convert the joined string into a set to retrieve the unique characters (call <a class="reference external" href="https://www.w3schools.com/python/python_sets.asp">set</a>)</p></li>
<li><p>Return a list of emmision propabilities dictionaries for all the texts (call <a class="reference internal" href="#forward.character_propabilities" title="forward.character_propabilities"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward.character_propabilities()</span></code></a>)</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameter<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>all_texts</strong> â€“ A list of texts</p>
</dd>
<dt class="field-even">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of dictionaries with emmision propabilities for each text</p>
</dd>
</dl>
</dd></dl>

<p>indem Sie den TODO-Anweisungen innerhalb der Methode folgen. Verwenden Sie
<a class="reference external" href="https://www.w3schools.com/python/ref_string_join.asp">join</a>.</p>
<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_emmision_propabilities</span><span class="p">(</span><span class="n">all_texts</span><span class="p">):</span>
  <span class="c1"># Join all texts and clean them</span>
  <span class="n">joined_text</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">all_texts</span><span class="p">))</span>

  <span class="c1"># Get a unique list of all characters across all five texts</span>
  <span class="n">all_chars</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">joined_text</span><span class="p">)</span>

  <span class="c1"># Now get the character emmision propabilities for each text</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">character_propabilities</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">all_chars</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">all_texts</span><span class="p">]</span>
</pre></div>
</div>
</div>
</section>
<section id="schritt-4-der-initiale-alpha-vektor">
<h2><strong>Schritt 4</strong>: Der initiale Alpha-Vektor<a class="headerlink" href="#schritt-4-der-initiale-alpha-vektor" title="Link to this heading">ïƒ</a></h2>
<p>Wir werden den VorwÃ¤rts-Algorithmus in vektorisierter Form implementieren, d.h. wir
berechnen die geschÃ¤tzten Zustandswahrscheinlichkeiten (die Alpha-Werte <span class="math notranslate nohighlight">\(\alpha_t\)</span>) als
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.array.html">np.array</a>. Da unser
Hidden Markov Model fÃ¼nf diskrete ZustÃ¤nde verwaltet (die fÃ¼nf Texte aus denen vorgelesen werden kann)
ist dieser Vektor fÃ¼nf-dimensional. Um den rekursiven Algorithmus zu starten benÃ¶tigen wir initiale
Werte fÃ¼r ebendiese Alpha-Werte. In unserem konkreten Kontext wissen wir nicht mit welchem Text
der Kollege zu lesen beginnt, die ZustÃ¤nde <span class="math notranslate nohighlight">\(x_1, \dots, x_5\)</span> sind also alle gleichwahrscheinlich, d.h.</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\alpha}_0 = (1, 1, 1, 1, 1)\]</div>
<p>Implementieren Sie nun die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.get_initial_alpha">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">get_initial_alpha</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#get_initial_alpha"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.get_initial_alpha" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>:
Return the initial alpha vector for the forward algorithm.</p>
<p>Hint: In the beginning, all states are equally likely</p>
<dl class="field-list simple">
<dt class="field-odd">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.array of shape 5x1 with the initial (equally likely) alpha values.</p>
</dd>
</dl>
</dd></dl>

<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_initial_alpha</span><span class="p">():</span>
  <span class="c1"># In the begining, we donÂ´t know which text our colleague choose</span>
  <span class="c1"># to start with, so all texts are equally likely</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</section>
<section id="schritt-5-die-zustandsubergangsmatrix">
<h2><strong>Schritt 5</strong>: Die ZustandsÃ¼bergangsmatrix<a class="headerlink" href="#schritt-5-die-zustandsubergangsmatrix" title="Link to this heading">ïƒ</a></h2>
<p>Im Laufe des VorwÃ¤rts-Algorthmus mÃ¼ssen wir den Term</p>
<div class="math notranslate nohighlight">
\[\sum_{x_{t-1}}  p(x_t | x_{t-1}) \alpha_t(x_{t-1})\]</div>
<p>berechnen. Dabei summiert die Summe Ã¼ber alle mÃ¶glichen ZustÃ¤nde <span class="math notranslate nohighlight">\(x_{t-1}\)</span>, in unserem Fall also
alle fÃ¼nf Text. Die Ãœbergangswahrscheinlichkeiten sind dabei derart das mit 90% Wahrscheinlichkeit der
selbe Text wieder gewÃ¤hlt wird wÃ¤hrend die restlichen 10% gleichmÃ¤ÃŸig auf die vier verbleibenden Text
aufgeteilt werden. FÃ¼r z.B. <span class="math notranslate nohighlight">\(x_t = 1\)</span>, also den ersten Text lÃ¤ÃŸt sich die Summe als Skalarprodukt</p>
<div class="math notranslate nohighlight">
\[\alpha_t(1) = (0.9, 0.025, 0.025, 0.025, 0.025)\cdot \boldsymbol{\alpha_{t-1}}\]</div>
<p>und entsprechend</p>
<div class="math notranslate nohighlight">
\[\alpha_t(2) = (0.025, 0.9, 0.025, 0.025, 0.025)\cdot \boldsymbol{\alpha_{t-1}}\]</div>
<p>etc. Der ZustandsÃ¼bergang vom alten Alpha-Vektor <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_{t-1}\)</span> zum neuen Alpha-Vektor <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_t\)</span>
lÃ¤ÃŸt sich demnach als Matrixmultiplikation ausdrÃ¼cken.</p>
<p>Implementieren Sie nun die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.get_state_transition_matrix">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">get_state_transition_matrix</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#get_state_transition_matrix"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.get_state_transition_matrix" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>:
Return the state transition matrix for the forward algorithm.</p>
<p>Hint: With 90% chance the state stays the same while the remaining 10% shall be equally divided between the four other states.</p>
<dl class="field-list simple">
<dt class="field-odd">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.array of shape 5x5 with the correct state transition propabilities</p>
</dd>
</dl>
</dd></dl>

<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_state_transition_matrix</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.900</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.900</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.900</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.900</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.900</span><span class="p">],</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
</section>
<section id="schritt-6-ein-schritt-im-vorwarts-algorithmus">
<h2><strong>Schritt 6</strong>: Ein Schritt im VorwÃ¤rts-Algorithmus<a class="headerlink" href="#schritt-6-ein-schritt-im-vorwarts-algorithmus" title="Link to this heading">ïƒ</a></h2>
<p>Was nun noch zu tun bleibt ist einen konkreten Schritt im VorswÃ¤rtsalgorithmus auszurechnen, also konkret</p>
<div class="math notranslate nohighlight">
\[\alpha_t(x_t) = p(y_t | x_t) \sum_{x_{t-1}}
p(x_t | x_{t-1})
\alpha_t(x_{t-1})\]</div>
<p>fÃ¼r alle ZustÃ¤nde <span class="math notranslate nohighlight">\(x_t = 1, 2, 3, 4, 5\)</span> zu berechnen. FÃ¼r die Summe haben wir bereits argumentiert
das sich diese als Matrixmultiplikation zwischen dem Alpha-Vektor sowie der ZustandsÃ¼bergangsmatrix (siehe oben) darstellen lÃ¤ÃŸt.</p>
<p>Der Term <span class="math notranslate nohighlight">\(p(y_t | x_t)\)</span> entspricht nun der Wahrscheinlichkeit fÃ¼r das Auftreten (observierens, beobachten) des Zeichen
<span class="math notranslate nohighlight">\(y_t\)</span> wenn wir im Zustand <span class="math notranslate nohighlight">\(x_t\)</span> sind. Hier konkret also die Wahrscheinlichkeit fÃ¼r ein bestimmtes Zeichen
gegen den konkreten Text aus dem es stammt. Diese Wahrscheinlichkeiten haben wir in den Methoden
<a class="reference internal" href="#forward.character_propabilities" title="forward.character_propabilities"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward.character_propabilities()</span></code></a> und <a class="reference internal" href="#forward.get_emmision_propabilities" title="forward.get_emmision_propabilities"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward.get_emmision_propabilities()</span></code></a> bereits berechnet.</p>
<p>Implementieren Sie nun die Methode</p>
<dl class="py function">
<dt class="sig sig-object py" id="forward.forward">
<span class="sig-prename descclassname"><span class="pre">forward.</span></span><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">character</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_transition_matrix</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">emmision_propabilities</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/forward.html#forward"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#forward.forward" title="Link to this definition">ïƒ</a></dt>
<dd><p><strong>TODO</strong>: Implement one step of the forward algorithm.</p>
<ul class="simple">
<li><p>Given the past alpha-values and the newly read character, use the state_transition_matrix</p></li>
</ul>
<p>to first predict the new state propabilities (new alpha values) according to the script.</p>
<ul class="simple">
<li><p>Then multiply the state propabilities with the emmision propabilities of the observed character</p></li>
</ul>
<p>for each alphabet to retrieve the new alpha values.</p>
<ul class="simple">
<li><p>Normalize the alpha vector after each step by diving by its sum. This helps to achieve numerically more stable results</p></li>
</ul>
<p>and allows for better interpretation of the results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameter<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> â€“ np.array of shape (5,1) holding the past alpha values</p></li>
<li><p><strong>character</strong> â€“ Observed character in this step</p></li>
<li><p><strong>state_transition_matrix</strong> â€“ np.array of shape (5,5) holding the state transition propabilities</p></li>
<li><p><strong>emmision_propabilities</strong> â€“ List of dictionaries holding the character emmision propabilities for each alphabet.</p></li>
</ul>
</dd>
<dt class="field-even">RÃ¼ckgabe<span class="colon">:</span></dt>
<dd class="field-even"><p>New alpha-vector after state transition and observation update (np.array of shape 5,1)</p>
</dd>
</dl>
</dd></dl>

<p>und folgen Sie den TODO-Anweisungen.</p>
<div class="toggle admonition">
<p class="admonition-title">LÃ¶sung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">character</span><span class="p">,</span> <span class="n">state_transition_matrix</span><span class="p">,</span> <span class="n">emmision_propabilities</span><span class="p">):</span>
  <span class="c1"># TODO: Implement state transition and update the alpha vector accordingly</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">state_transition_matrix</span> <span class="o">@</span> <span class="n">alpha</span>

  <span class="c1"># TODO: Retrieve symbol emmision propabilties for the given character and update the alpha vector</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">alphabet</span><span class="p">[</span><span class="n">character</span><span class="p">]</span> <span class="k">for</span> <span class="n">alphabet</span> <span class="ow">in</span> <span class="n">emmision_propabilities</span><span class="p">])</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">*</span> <span class="n">alpha</span>

  <span class="c1"># TODO: Normalize alpha for better visualization (divide by sum)</span>
  <span class="n">alpha</span> <span class="o">/=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

  <span class="c1"># TODO: Return alpha</span>
  <span class="k">return</span> <span class="n">alpha</span>
</pre></div>
</div>
</div>
</section>
<section id="ergebnisse">
<h2>Ergebnisse<a class="headerlink" href="#ergebnisse" title="Link to this heading">ïƒ</a></h2>
<p>Wenn Sie alles richtig implemeniert haben sehen Sie eine Grafik welche die Verteilung der
geschÃ¤tzten Zustandswahrscheinlichkeiten (Alpha-Werte) nach jedem einzelnen Zeichen zeigt.
Dunklere KÃ¤stchen entsprechen dabei hÃ¶heren Wahrscheinlichkeiten fÃ¼r den jeweiligen Zustand (Text).</p>
<a class="reference internal image-reference" href="../_images/alphaovertime.png"><img alt="Alpha Ã¼ber Zeitpunkt" class="align-center" src="../_images/alphaovertime.png" style="width: 800px;" />
</a>
<p>Es ist schÃ¶n zu sehen das zunÃ¤chst (fÃ¼r die ersten 25 Schritte) der erste Text am wahrscheinlichsten ist.
Text 4 ist jedoch Ã¤hnlich wahrscheinlich. Etwa zu Schritt 25 schÃ¤tzt das Modell nun das der <em>hidden state</em>
(also der Text, aus dem vorgelesen wurde) sich zu Text 4 verÃ¤ndert hat weil die Wahrscheinlichkeit fÃ¼r Text 1
auf nahezu 0 sinkt. Schaut man in die Ã¼bermittelte Sequenz so steht an 25ter Stelle das Symbol ğŸ˜’.
Dieses kommt im vierten Text tatsÃ¤chlich recht hÃ¤ufig vor wÃ¤hrend es im ersten Text nur ein einziges mal auftaucht.
Diese Beobachtung scheint also ausschlaggebend dafÃ¼r zu sein das das Modell die Zustandswahrscheinlichkeiten
drastisch verÃ¤ndert und sich fÃ¼r Text 4 entscheidet.</p>
<p>Zwischen Schritt 30 und 40 konvergiert das Modell immer stÃ¤rker zu der Vermutung das nun aus Text 2 gelesen wird.
Diese Vermutung hÃ¤lt sich auch bis zum Ende der Sequenz so das ihre SchÃ¤tzung in dem Eingangs erwÃ¤hnten Spiel mit ihrem Kollegen
also wÃ¤re, das dieser am Ende aus dem zweiten Text liest. Der VorwÃ¤rts-Algorithmus erlaubt es die ZustandsÃ¼bergÃ¤nge
und die Beobachtung so miteinander zu kombinieren das wir den wahrscheinlichsten Zustand ermitteln kÃ¶nnen.</p>
</section>
<section id="musterlosung">
<h2>MusterlÃ¶sung<a class="headerlink" href="#musterlosung" title="Link to this heading">ïƒ</a></h2>
<p><a class="reference internal" href="source.html"><span class="doc">VorswÃ¤rtsalgorithmus - MusterlÃ¶sung</span></a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../multivariate/index.html" class="btn btn-neutral float-left" title="Minimum Variance Fusion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> ZurÃ¼ck</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Prof. Dr. Dennis MÃ¼ller.</p>
  </div>

  Erstellt mit <a href="https://www.sphinx-doc.org/">Sphinx</a> mit einem
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    bereitgestellt von <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>